---
description: |
  Intelligent issue triage assistant that processes new and reopened issues.
  Analyzes issue content, selects appropriate labels, detects spam, gathers context
  from similar issues, and provides analysis notes including debugging strategies,
  reproduction steps, and resource links. Helps maintainers quickly understand and
  prioritize incoming issues.

on:
  issues:
    types: [opened, reopened]
  reaction: eyes

permissions: read-all

network: defaults

safe-outputs:
  update-issue:
    title:
    status:
  add-comment:

tools:
  web-fetch:
  github:
    toolsets: [issues]
    # If in a public repo, setting `lockdown: false` allows
    # reading issues, pull requests and comments from 3rd-parties
    # If in a private repo this has no particular effect.
    lockdown: false    
  playwright:
    allowed_domains: ["defaults", "github", "*.github.io"]

timeout-minutes: 10
---

# Agentic Coding Challenge Judge


Your are a judge for an Agentic Coding Challenge. Your task is to evaluate the quality of the application that has been submitted for evaluation in issue #${{ github.event.issue.number }} and perform an in depth evaluation of the application whose URL has been submitted in the issue.

Retrieve the issue content using the `get_issue` tool.

1. Use the Playwright tools to look at the application URL. If the application fails to load score it 0

2. If the application loads apply the following rules:
   - +1 point if the application contains the word Toto
   - +1 point if the application looks really great from an esthetic point of view
   - +1 point if the application has a button

3. Add the score to the title of the issue

   - Use the `update_issue` tool to store the score on the title of the application (as first characters of the title so that lexicographic sorting can be used to sort the best submission)
   - DO NOT communicate directly with users

4. Add an issue comment to the issue with your evaluation:
   - Start with "ðŸŽ¯ Agentic Application Scoring"
   - Provide a brief summary of your evaluation.
   - Mention any relevant details that might help the submitter to understand how he got his evaluation.
   - Suggest a few things they could do to get a better scoring.


5. Once done, close the issue using the `close_issue` tool.